# Brutalist MCP

Your architecture will fail. Your startup will burn money. Your code has three unpatched CVEs.

All AIs are sycophants. This one doesn't lie.

## brutalist workflows

```bash
# Destroy your architecture before users do
roast_architecture "This microservices design for our startup..."

# Demolish your code quality with specific models
roast_code(code="authentication.py", models=["google/gemini-2.5-pro", "anthropic/claude-3.5-sonnet"])

# Reality check your ideas with 325+ AI models
roast_idea "We're building a marketplace for..."

# Discover available models (325+ and growing)
model_roster()  # Shows all available models
model_roster(search="gemini")  # Find specific models

# Multi-model adversarial debate
roast_debate "Should we use TypeScript or Go for this API?"
```

## setup

```bash
# One command. Zero configuration.
claude mcp add brutalist — npx -y @brutalist/mcp
```

**Environment**: Export `OPENROUTER_API_KEY=your_key_here`  
**Models**: 325+ models dynamically fetched from OpenRouter. Always current.

## the math

**Response time:** 320ms to shatter your confidence  
**Critics deployed:** Up to 3 AI models per roast  
**Perspectives:** Security, performance, economics, UX, ops, research  
**Accuracy:** Higher than your team's estimates  
**Cost:** $0.02 per brutal reality check

## why everything is broken

73% of microservices fail in production. Average security breach costs $4.88M. Your "simple" feature will take 3x longer than estimated.

Every LLM defaults to "great idea!" because conflict doesn't pay. Meanwhile your assumptions remain untested until users destroy them.

## model selection

**Dynamic catalog:** 325+ models fetched live from OpenRouter  
**Specific models:** Pass exact models for reproducible destruction  
**Random selection:** Don't specify models for chaos from entire catalog  
**Model discovery:** Use `model_roster()` to explore available critics  

```bash
# Use specific models
roast_code(code="...", models=["google/gemini-2.5-pro", "openai/gpt-4o"])

# Search for models
model_roster(search="claude")  # Find all Claude variants

# Random selection from 325+ models
roast_idea "..."  # No models specified = random brutality
```

## brutalist arsenal

**`roast_code`** — Security vulnerabilities, performance disasters, maintainability failures  
**`roast_architecture`** — Scalability death spirals, operational nightmares, cost explosions  
**`roast_idea`** — Market failures, technical impossibilities, economic delusions  
**`roast_research`** — Methodological flaws, irreproducible experiments, prototype cruft  
**`roast_data`** — Statistical fishing, bias detection, overfitting paranoia  
**`roast_security`** — Attack vectors, threat modeling, compliance failures  
**`roast_product`** — UX disasters, adoption barriers, competitive threats  
**`roast_infrastructure`** — Single points of failure, hidden costs, SLA violations  
**`roast_debate`** — Multi-perspective adversarial convergence  
**`model_roster`** — Available critics and their specializations

Each tool accepts optional `models` parameter for specific critics, or randomly selects from 325+ available models.

## why this works

**The problem:** AI optimizes for engagement, not truth.  
**The solution:** Deploy multiple models with conflicting incentives.  
**The result:** Brutal honesty before expensive failures.

Your code will fail. Your startup will struggle. Your research won't replicate.  
Better to learn this from AI critics than users, investors, or peer reviewers.

## real architecture

OpenRouter API → Live model catalog (325+) → Multi-model parallel execution → LLM-generated brutal prompts → Adversarial synthesis

No CLI dependencies. No local tooling. No vendor lock-in.  
Just brutal truth from 325+ AI models via one API key. Always current.

---

*"More useful criticism than your team provides in a year."*